import os
import re
import json
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
import cv2

import torch
from torch.utils.data import Dataset

USE_SEQUENCE_LABEL = False  # âœ… æ§åˆ¶æ˜¯å¦ä½¿ç”¨åºåˆ—æ ‡ç­¾ï¼ˆFalse = ä½¿ç”¨å¸§æ ‡ç­¾ï¼‰


class ThermalBlinkDataset(Dataset):
    def __init__(
        self,
        pkl_root: str,
        csv_root: str,
        subfolders: list,
        val_pkl_dir: str = None,
        val_csv_dir: str = None,
        is_val: bool = False,
        center_size: tuple = (12, 16),
        sequence_length: int = 32
    ):
        """
        Args:
            pkl_root: çƒ­å›¾æ•°æ®æ ¹ç›®å½•
            csv_root: blinkæ ‡æ³¨csvæ–‡ä»¶ç›®å½•
            subfolders: å­æ–‡ä»¶å¤¹åˆ—è¡¨ï¼Œå¦‚ ["0503", "0505"]
            val_pkl_dir: éªŒè¯é›†å•ç‹¬pklè·¯å¾„(is_val=Trueæ—¶å¯ç”¨)
            val_csv_dir: éªŒè¯é›†å¯¹åº”csvè·¯å¾„(is_val=Trueæ—¶å¯ç”¨)
            is_val: æ˜¯å¦æ˜¯éªŒè¯é›†
            center_size: ä¸­å¿ƒè£å‰ªåŒºåŸŸå¤§å°ï¼Œé»˜è®¤(12,16)
        """
        self.center_size = center_size
        self.data = []
        self.sequence_length = sequence_length

        if is_val:
            # åªåŠ è½½ä¸€å¯¹ val pkl + csv
            for filename in os.listdir(val_pkl_dir):
                if not filename.endswith(".pkl"):
                    continue
                pkl_path = os.path.join(val_pkl_dir, filename)
                match = re.match(r"(.*_\d{8}_\d{4})", filename)
                if not match:
                    continue
                fuzzy_key = match.group(1)
                matched_csvs = [
                    f for f in os.listdir(val_csv_dir)
                    if f.startswith("blink_offsets_") and fuzzy_key in f and f.endswith(".csv")
                ]
                if not matched_csvs:
                    continue
                csv_path = os.path.join(val_csv_dir, matched_csvs[0])
                X, y, timestamps = self.process_sample(pkl_path, csv_path, return_timestamps=True)
                stride = self.sequence_length // 2
                if USE_SEQUENCE_LABEL:
                    for i in range(0, len(X) - self.sequence_length + 1, stride):
                        x_seq = X[i:i + self.sequence_length]
                        y_seq = y[i // stride]
                        self.data.append((x_seq, y_seq))
                else:
                    for i in range(len(X)):
                        self.data.append((X[i], y[i]))


        else:
            # æ‰¹é‡åŠ è½½è®­ç»ƒé›†
            for subfolder in subfolders:
                pkl_dir = os.path.join(pkl_root, subfolder)
                for filename in os.listdir(pkl_dir):
                    if not filename.endswith(".pkl"):
                        continue
                    pkl_path = os.path.join(pkl_dir, filename)
                    if val_pkl_dir and pkl_path == val_pkl_dir:
                        continue  # æ’é™¤éªŒè¯é›†æ–‡ä»¶
                    base_name = os.path.splitext(filename)[0]  # å»æ‰ .pkl åç¼€
                    parts = base_name.split('_')
                    fuzzy_key = parts[-3] + "_" + parts[-2][:4]


                    # å» gt_output çš„å­ç›®å½•ä¸­åŒ¹é…å¯¹åº” CSV
                    csv_subdir = os.path.join(csv_root, subfolder)
                    if not os.path.exists(csv_subdir):
                        print(f"[WARN] CSV å­ç›®å½•ä¸å­˜åœ¨: {csv_subdir}")
                        continue
                    matched_csvs = [
                        f for f in os.listdir(csv_subdir)
                        if f.startswith("blink_offsets_") and fuzzy_key in f and f.endswith(".csv")
                    ]
                    if not matched_csvs:
                        print(f"[WARN] æœªåŒ¹é…åˆ° CSV: {fuzzy_key} in {csv_subdir}")
                        continue
                    csv_path = os.path.join(csv_subdir, matched_csvs[0])

                    X, y, timestamps = self.process_sample(pkl_path, csv_path, return_timestamps=True)

                    if X is not None:
                        stride = self.sequence_length // 2
                        if USE_SEQUENCE_LABEL:
                            for i in range(0, len(X) - self.sequence_length + 1, stride):
                                x_seq = X[i:i + self.sequence_length]
                                y_seq = y[i // stride]
                                ts_seq = timestamps[i:i + self.sequence_length]
                                self.data.append((x_seq, y_seq, ts_seq))
                        else:
                            for i in range(len(X)):
                                self.data.append((X[i], y[i], timestamps[i]))



    def __len__(self):
        return len(self.data) - self.sequence_length + 1

    def __getitem__(self, idx):
        seq_len = self.sequence_length

        if idx + seq_len > len(self.data):
            idx = len(self.data) - seq_len

        frames, labels, timestamps = [], [], []

        for i in range(seq_len):
            item = self.data[idx + i]
            x, y = item[:2]
            frames.append(torch.tensor(x).float().permute(2, 0, 1))  # [C, H, W]
            labels.append(torch.tensor(y).float())
            if len(item) == 3:
                timestamps.append(item[2])  # åªåœ¨éªŒè¯é›†æœ‰
                
                
        if USE_SEQUENCE_LABEL:
            x_seq = torch.stack(frames, dim=0)   # [T, C, H, W]
            y_seq = torch.tensor(labels[0]).float()  # å•ä¸ªæ ‡ç­¾
        else:

            x_seq = torch.stack(frames, dim=0)   # [T, C, H, W]
            y_seq = torch.stack(labels, dim=0)   # [T]

        if timestamps:
            return {"x": x_seq, "y": y_seq, "timestamp": torch.tensor(timestamps)}
        else:
            return {"x": x_seq, "y": y_seq}
        
        
    def assign_frame_labels(self, timestamps, blink_start_offsets, blink_end_offsets):
        """
        æ ‡ç­¾æ–¹å¼1ï¼šæŒ‰å¸§æ‰“æ ‡ç­¾ï¼Œstart ~ end åŒºé—´ä¸º 1ï¼Œå…¶ä»–ä¸º 0
        """
        labels = np.zeros_like(timestamps, dtype=np.float32)
        for start, end in zip(blink_start_offsets, blink_end_offsets):
            labels[(timestamps >= start) & (timestamps <= end)] = 1.0
        return labels

    def assign_sequence_labels(self, frame_labels, sequence_length=32, stride=16):
        """
        æ ‡ç­¾æ–¹å¼2ï¼šåŸºäºå¸§æ ‡ç­¾ç”Ÿæˆåºåˆ—æ ‡ç­¾ã€‚
        è‹¥åºåˆ—ä¸­æ—¢æœ‰0åˆæœ‰1ï¼Œåˆ™ä¸º1ï¼›å¦åˆ™ä¸º0ã€‚
        """
        sequence_labels = []
        for i in range(0, len(frame_labels) - sequence_length + 1, stride):
            segment = frame_labels[i:i + sequence_length]
            label = 1.0 if np.any(segment != segment[0]) else 0.0
            sequence_labels.append(label)
        return np.array(sequence_labels, dtype=np.float32)
    


    def process_sample(self, pkl_path, csv_path, return_timestamps=False):
        try:
            with open(pkl_path, "rb") as f:
                data = pickle.load(f)
            if not ('temperature' in data and 'timestamp' in data):
                print(f"[ERROR] ç¼ºå­—æ®µ: {pkl_path}")
                return None, None, None
        except Exception as e:
            print(f"[ERROR] pkl è¯»å–å¤±è´¥: {pkl_path}\n{e}")
            return None, None, None

        try:
            df = pd.read_csv(csv_path)
            offsets = {row['key']: json.loads(row['value']) for _, row in df.iterrows()}
            blink_start_offsets = offsets["start_offsets"]
            blink_end_offsets = offsets["end_offsets"]
            # âœ… å¦‚æœèµ·å§‹æ•°é‡ä¸ä¸€è‡´ï¼Œä¸” end çš„ç¬¬ä¸€å¸§åœ¨ start ä¹‹å‰ï¼Œåˆ™è¡¥ 0
            if len(blink_start_offsets) != len(blink_end_offsets):
                print(f"[WARN] ä¿®æ­£ä¸­: {os.path.basename(pkl_path)}")
                
                # ğŸ‘‰ å¦‚æœ end æ¯” start å¤šï¼Œè¯´æ˜ç¼ºå°‘èµ·å§‹ï¼Œè¡¥ 0
                if len(blink_end_offsets) > len(blink_start_offsets):
                    blink_start_offsets = [0] + blink_start_offsets
                    print(f"â• æ’å…¥ start=0")

                # ğŸ‘‰ å¦‚æœ start æ¯” end å¤šï¼Œè¯´æ˜ç¼ºå°‘ç»“æŸï¼Œè¡¥æœ€åä¸€å¸§æ—¶é—´
                elif len(blink_start_offsets) > len(blink_end_offsets):
                    last_offset = (datetime.fromisoformat(data['timestamp'][-1]) - datetime.fromisoformat(data['timestamp'][0])).total_seconds() * 1000
                    blink_end_offsets.append(int(last_offset))
                    print(f"â• è¡¥å…… end={int(last_offset)}")

                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å¯¹é½
                if len(blink_start_offsets) != len(blink_end_offsets):
                    print(f"[ERROR] ä¿®æ­£åä»ä¸ä¸€è‡´: {pkl_path}")
                    print(len(blink_start_offsets), len(blink_end_offsets))
                    return None, None, None



        except Exception as e:
            print(f"[ERROR] CSV è¯»å–å¤±è´¥: {csv_path}\n{e}")
            return None, None, None

        # temperature_frames = np.array(data['temperature'])  # [N, 12, 16]
        raw_frames = np.array(data['temperature'])  # [N, H, W]
        
        # âœ… Step A: ç»Ÿä¸€é¢„å¤„ç†å¹¶ clip åˆ°å›ºå®šèŒƒå›´ [-3, 3]
        enhanced_all = []
        for frame in raw_frames:
            # â‘  é«˜æ–¯æ»¤æ³¢
            blurred = cv2.GaussianBlur(frame, (3, 3), sigmaX=0.5)
            # â‘¡ æ ‡å‡†å·®å½’ä¸€åŒ–
            enhanced = (blurred - np.mean(blurred)) / (np.std(blurred) + 1e-5)
            enhanced_all.append(enhanced)

        # âœ… å›ºå®š clip èŒƒå›´
        global_min = -3.0
        global_max = 2.0
        print(f"ğŸŒ¡ï¸ ä½¿ç”¨ç»Ÿä¸€å¢å¼ºèŒƒå›´: min={global_min}, max={global_max}")

        # âœ… Step B: clip + gamma + CLAHE
        processed_frames = []
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2, 2))  # â‘£ CLAHE åˆå§‹åŒ–

        for enhanced in enhanced_all:
            # âœ… Clip åˆ°ç»Ÿä¸€èŒƒå›´
            clipped = np.clip(enhanced, global_min, global_max)

            # âœ… å½’ä¸€åŒ–å¹¶ gamma æ‹‰ä¼¸
            norm_0_1 = (clipped - global_min) / (global_max - global_min)
            adjusted = np.power(norm_0_1, 0.5)

            # âœ… æ˜ å°„åˆ° [0, 255] å¹¶ CLAHE
            norm = (adjusted * 255).astype(np.uint8)
            contrast_enhanced = clahe.apply(norm)

            processed_frames.append(contrast_enhanced)


        temperature_frames = np.stack(processed_frames, axis=0)  # [N, H, W]

        
        raw_timestamps = data['timestamp']
        parsed_times = [datetime.fromisoformat(ts) for ts in raw_timestamps]
        start_time = parsed_times[0]
        timestamps = np.array([(t - start_time).total_seconds() * 1000 for t in parsed_times])  # ms
        
        # âœ… åˆå¹¶ç›¸é‚»çœ¨çœ¼æ®µï¼ˆé—´éš”å°äºç­‰äº1000msï¼‰
        merged_starts, merged_ends = [], []
        if blink_start_offsets:
            cur_start = blink_start_offsets[0]
            cur_end = blink_end_offsets[0]
            for i in range(1, len(blink_start_offsets)):
                next_start = blink_start_offsets[i]
                next_end = blink_end_offsets[i]
                if next_start - cur_end <= 1000:
                    cur_end = max(cur_end, next_end)
                else:
                    merged_starts.append(cur_start)
                    merged_ends.append(cur_end)
                    cur_start = next_start
                    cur_end = next_end
            merged_starts.append(cur_start)
            merged_ends.append(cur_end)
            blink_start_offsets = merged_starts
            blink_end_offsets = merged_ends

        
        # å¸§çº§æ ‡ç­¾
        if USE_SEQUENCE_LABEL:
            labels = self.assign_sequence_labels(
                self.assign_frame_labels(timestamps, blink_start_offsets, blink_end_offsets),
                sequence_length=self.sequence_length,
                stride=self.sequence_length // 2
            )
        else:
            labels = self.assign_frame_labels(timestamps, blink_start_offsets, blink_end_offsets)


        # ä¸­å¿ƒè£å‰ª
        h, w = temperature_frames[0].shape
        ch, cw = self.center_size
        sr = h // 2 - ch // 2
        er = sr + ch
        sc = w // 2 - cw // 2
        ec = sc + cw
        cropped_frames = temperature_frames[:, sr:er, sc:ec]  # [N, H', W']
        X = cropped_frames[..., np.newaxis]  # [N, H', W', 1]
        
        # X = temperature_frames[..., np.newaxis]  # [N, H, W, 1]


        if return_timestamps:
            return X, labels, timestamps
        else:
            return X, labels, None





# import matplotlib.pyplot as plt

# def create_composite(images, cols=50, resize_shape=(160, 120)):
#     if len(images) == 0:
#         return None
#     rows = (len(images) + cols - 1) // cols
#     width, height = resize_shape
#     canvas = np.zeros((rows * height, cols * width), dtype=np.uint8)
#     for idx, img in enumerate(images):
#         r, c = divmod(idx, cols)
#         y0, y1 = r * height, (r + 1) * height
#         x0, x1 = c * width, (c + 1) * width
#         canvas[y0:y1, x0:x1] = cv2.resize(img, resize_shape, interpolation=cv2.INTER_NEAREST)
#     return canvas


# def visualize_processed_frames(frames, group_size=160, cols=50, resize_shape=(160, 120)):
#     gap_height = 20
#     rows = []
#     for i in range(0, len(frames), group_size):
#         group = frames[i:i + group_size]
#         group_canvas = create_composite(group, cols=cols, resize_shape=resize_shape)
#         rows.append(group_canvas)
#         gap = np.zeros((gap_height, group_canvas.shape[1]), dtype=np.uint8)
#         rows.append(gap)
#     final_canvas = cv2.vconcat(rows[:-1]) if len(rows) > 1 else rows[0]
#     plt.figure(figsize=(20, 12))
#     plt.imshow(final_canvas, cmap='jet')
#     plt.title("Thermal Frames (æ¯160å¸§ä¸€ç»„ï¼Œç»„é—´éš”ç©ºè¡Œ)")
#     plt.axis('off')
#     plt.tight_layout()
#     plt.show()


# def main():
#     # âœ… ä¿®æ”¹è·¯å¾„ä¸ºä½ å®é™…çš„éªŒè¯é›†æ ·æœ¬è·¯å¾„
#     val_pkl_path = "/Users/yvonne/Documents/final project/ThermalEye/ira_data/0505/callibration_20250505_161542_107.pkl"
#     val_csv_path = "/Users/yvonne/Documents/final project/ThermalEye/gt_output/0505/blink_offsets_callibration_20250505_161542_482.csv"

#     # âœ… åªåŠ è½½è¿™ä¸€å¯¹æ ·æœ¬ç”¨äºå¯è§†åŒ–
#     dataset = ThermalBlinkDataset(
#         pkl_root=None, csv_root=None, subfolders=[],
#         val_pkl_dir=os.path.dirname(val_pkl_path),
#         val_csv_dir=os.path.dirname(val_csv_path),
#         is_val=True
#     )

#     # âœ… æå–åŸå§‹å¢å¼ºåçš„å¸§ï¼ˆä» process_sample å†è·‘ä¸€æ¬¡ï¼‰
#     frames, _, _ = dataset.process_sample(val_pkl_path, val_csv_path, return_timestamps=True)

#     # âœ… frames.shape = [N, H, W, 1]ï¼Œå…ˆ squeeze å¹¶å˜ list
#     images = [f.squeeze() for f in frames]  # -> List of [H, W]

#     # âœ… å±•ç¤º
#     visualize_processed_frames(images)

# if __name__ == "__main__":
#     main()
